{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05fc3be-0651-486d-ac53-0c28f22a434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, json, time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42d33c8-8eee-486f-9fb9-12af8d8401c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pages:14,\n",
      "Priority:['about', 'company', 'product', 'pricing', 'contact', 'careers']\n"
     ]
    }
   ],
   "source": [
    "PRIORITY_KEYWORDS = [\"about\",\"company\",\"product\",\"pricing\",\"contact\",\"careers\"]\n",
    "MAX_PAGES = 14 #As prescribes before in the documents as limited pages so it's set upto 10 - 15 pages\n",
    "TIMEOUT = (5, 10)\n",
    "HEADERS = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "}\n",
    "print(f\"Max pages:{MAX_PAGES},\\nPriority:{PRIORITY_KEYWORDS[:7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "382e142f-d318-4b30-ad71-fcd8ad094328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your url here or paste here https://www.truemeds.in/\n"
     ]
    }
   ],
   "source": [
    "# Give Your Url at here for the \"url\"\n",
    "#url=\"https://en.wikipedia.org/wiki/Cognizant\"  # Enter your URL here \n",
    "url=input('enter your url here or paste here')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b578764f-e471-4bf2-af1e-a7b2f8ac3ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fetched 914914 chars from url\n"
     ]
    }
   ],
   "source": [
    "def fetch_url(url):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            return resp.text\n",
    "        except:\n",
    "            if attempt == 2:\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    return None\n",
    "\n",
    "# Testing the fetcher\n",
    "test_html = fetch_url(url)\n",
    "print(f\"   Fetched {len(test_html) if test_html else 0} chars from url\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b83f7d1-2806-4a63-9575-f76f8d324eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing and verifying the crawler work\n",
    "def priority_score(url):\n",
    "    path = urlparse(url).path.lower()\n",
    "    return sum(3 for kw in PRIORITY_KEYWORDS if kw in path)\n",
    "\n",
    "def crawl(start_url):\n",
    "    queue = deque([start_url])\n",
    "    visited = set()\n",
    "    pages = {}\n",
    "    errors = []\n",
    "    \n",
    "    while queue and len(visited) < MAX_PAGES:\n",
    "        # Pick highest priority URL\n",
    "        url = max(queue, key=priority_score)\n",
    "        queue.remove(url)\n",
    "        \n",
    "        if url in visited: continue\n",
    "            \n",
    "        print(f\"[{len(visited)+1}/{MAX_PAGES}] {url}\")\n",
    "        html = fetch_url(url)\n",
    "        \n",
    "        if html is None:\n",
    "            errors.append({\"url\": url, \"error\": \"failed\"})\n",
    "            visited.add(url)\n",
    "            continue\n",
    "            \n",
    "        visited.add(url)\n",
    "        pages[url] = html\n",
    "        \n",
    "        # Finding thed internal links ok the url given\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = urljoin(url, a['href'])\n",
    "            if (urlparse(href).netloc == urlparse(start_url).netloc and \n",
    "                href not in visited and href not in queue):\n",
    "                queue.append(href)\n",
    "    \n",
    "    return pages, list(visited),errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18008423-1297-4ab0-b0d8-0b89137a447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/14] https://www.truemeds.in/\n",
      "[2/14] https://www.truemeds.in/blog/category/well-being/pregnancy-reproduction\n",
      "[3/14] https://www.truemeds.in/about-us\n",
      "[4/14] https://www.truemeds.in/blog/category/lifestyle/product-reviews\n",
      "[5/14] https://www.truemeds.in/all-medicine-list\n",
      "[6/14] https://www.truemeds.in/categories/personal-care-1\n",
      "[7/14] https://www.truemeds.in/categories/personal-care/skin-care-125\n",
      "[8/14] https://www.truemeds.in/categories/personal-care/hair-care-127\n",
      "[9/14] https://www.truemeds.in/categories/personal-care/baby-and-mom-care-122\n",
      "[10/14] https://www.truemeds.in/categories/personal-care/sexual-wellness-123\n",
      "[11/14] https://www.truemeds.in/categories/personal-care/oral-care-128\n",
      "[12/14] https://www.truemeds.in/categories/personal-care/elderly-care-124\n",
      "[13/14] https://www.truemeds.in/categories/personal-care/skin-care/skin-cream-60\n",
      "[14/14] https://www.truemeds.in/categories/personal-care/skin-care/sunscreen-61\n",
      "âœ… Crawled 14 pages, 0 errors\n"
     ]
    }
   ],
   "source": [
    "pages, visited, errors = crawl(url)\n",
    "print(f\"âœ… Crawled {len(pages)} pages, {len(errors)} errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3d85451-866e-4bed-ab8c-94f20af8ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Extracters to work properly and helping them with the initiations \n",
    "def extract_contacts(text):\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\w+', text)\n",
    "    phones = re.findall(r'(\\+?\\d[\\d\\s\\-\\(\\)]{7,})', text)\n",
    "    date_pattern= [r'\\d{1,2}-\\d{1,2}-\\d{4}',  \n",
    "        r'\\d{4}-\\d{1,2}-\\d{1,2}',  \n",
    "        r'\\d{2}/\\d{2}/\\d{4}'       \n",
    "    ]   \n",
    "    return list(set(emails))[:5], [p.strip() for p in phones if len(p)>9][:3]\n",
    "\n",
    "def extract_social(soup):\n",
    "    social = {}\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href'].lower()\n",
    "        if 'linkedin' in href: social['linkedin'] = a['href']\n",
    "        if 'twitter.com' in href or 'x.com' in href: social['twitter'] = a['href']\n",
    "    return social\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e1dcc7-e52a-4ac5-aaca-b7ddfe199e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_json(pages, visited, errors, start_url):\n",
    "    # Creating the JSON files based on prefered strucure\n",
    "    record = {\n",
    "        \"identity\": {\"name\": None, \"url\": start_url, \"tagline\": None},\n",
    "        \"business\": {\"description\": [], \"offerings\": []},\n",
    "        \"contact\": {\"emails\": [], \"phones\": []},\n",
    "        \"pages\": {\"key_pages\": {}, \"social\": {}},\n",
    "        \"meta\": {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"pages_crawled\": visited,\n",
    "            \"errors\": errors\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extracting the information from all pages as extracted\n",
    "    all_emails,all_phones,all_social =[],[],{}\n",
    "    \n",
    "    for url, html in pages.items():\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        \n",
    "        # Extracting Company name from title\n",
    "        title = soup.title.string.strip() if soup.title else \"\"\n",
    "        if not record[\"identity\"][\"name\"] and title:\n",
    "            record[\"identity\"][\"name\"] = title.split(' | ')[0]\n",
    "        \n",
    "        # Extracting Contacts info\n",
    "        text = soup.get_text()\n",
    "        emails, phones = extract_contacts(text)\n",
    "        all_emails.extend(emails)\n",
    "        all_phones.extend(phones)\n",
    "        \n",
    "        #Extracting Social info\n",
    "        all_social.update(extract_social(soup))\n",
    "        \n",
    "        # Key_pages\n",
    "        path = urlparse(url).path.lower()\n",
    "        for kw in PRIORITY_KEYWORDS:\n",
    "            if kw in path:\n",
    "                record[\"pages\"][\"key_pages\"][kw] = url\n",
    "    \n",
    "    # Finalizing the tracker in order to give prior responses\n",
    "    record[\"contact\"][\"emails\"] = list(set(all_emails))\n",
    "    record[\"contact\"][\"phones\"] = list(set(all_phones))\n",
    "    record[\"pages\"][\"social\"] = all_social\n",
    "    \n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "509ba8e7-333a-45c2-8fa5-8d48d5a3dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_company(url):\n",
    "    \"\"\"Step 7: COMPLETE PIPELINE\"\"\"\n",
    "    print(f\"\\nðŸŽ¯ Scraping {url}\")\n",
    "    pages, visited, errors = crawl(url)\n",
    "    record = build_json(pages, visited, errors, url)\n",
    "    \n",
    "    # Save JSON file\n",
    "    filename = f\"{urlparse(url).netloc}_profile.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(record, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved: {filename}\")\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c64c094a-2e24-492d-a393-a99db7bf6e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Scraping https://www.truemeds.in/\n",
      "[1/14] https://www.truemeds.in/\n",
      "[2/14] https://www.truemeds.in/blog/category/well-being/pregnancy-reproduction\n",
      "[3/14] https://www.truemeds.in/about-us\n",
      "[4/14] https://www.truemeds.in/blog/category/lifestyle/product-reviews\n",
      "[5/14] https://www.truemeds.in/all-medicine-list\n",
      "[6/14] https://www.truemeds.in/categories/personal-care-1\n",
      "[7/14] https://www.truemeds.in/categories/personal-care/skin-care-125\n",
      "[8/14] https://www.truemeds.in/categories/personal-care/hair-care-127\n",
      "[9/14] https://www.truemeds.in/categories/personal-care/baby-and-mom-care-122\n",
      "[10/14] https://www.truemeds.in/categories/personal-care/sexual-wellness-123\n",
      "[11/14] https://www.truemeds.in/categories/personal-care/oral-care-128\n",
      "[12/14] https://www.truemeds.in/categories/personal-care/elderly-care-124\n",
      "[13/14] https://www.truemeds.in/categories/personal-care/skin-care/skin-cream-60\n",
      "[14/14] https://www.truemeds.in/categories/personal-care/skin-care/sunscreen-61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eswar\\AppData\\Local\\Temp\\ipykernel_33172\\2664331542.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: www.truemeds.in_profile.json\n",
      "Files created:\n"
     ]
    }
   ],
   "source": [
    "#By here the scraper creates the json files which required for the inspection using the fetcher \n",
    "summary=scrape_company(url)\n",
    "print(\"Files created:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "407f0ef5-697b-4a36-8e0e-cf19e04213e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Company Summary\n",
      "==================================================\n",
      "Name: Order Medicine Online\n",
      "Emails: 2\n",
      "Phones: 3\n",
      "Pages: 14\n"
     ]
    }
   ],
   "source": [
    "def show_summary(record):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Company Summary\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Name: {record['identity']['name'] or 'Not found'}\")\n",
    "    print(f\"Emails: {len(record['contact']['emails'])}\")\n",
    "    print(f\"Phones: {len(record['contact']['phones'])}\")\n",
    "    print(f\"Pages: {len(record['meta']['pages_crawled'])}\")\n",
    "show_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f8308-e412-40e1-beff-508d7e6fb29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
